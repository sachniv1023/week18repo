{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c66893c",
   "metadata": {},
   "source": [
    "#### 1.What is a neural network? What are the general steps required to build a neural network? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb63d53",
   "metadata": {},
   "source": [
    "A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through\n",
    "a process that mimics the way the human brain operates. \n",
    "In this sense, neural networks refer to systems of neurons, either organic or artificial in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc4d9c",
   "metadata": {},
   "source": [
    "Build a neural network in 7 steps\n",
    "\n",
    "    Create an approximation project.\n",
    "\n",
    "    Configure data set.\n",
    "\n",
    "    Set network architecture.\n",
    "\n",
    "    Train neural network.\n",
    "\n",
    "    Improve generalization performance.\n",
    "\n",
    "    Test results.\n",
    "\n",
    "    Deploy model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a22c69",
   "metadata": {},
   "source": [
    "#### 2.Generally, how do you check the performance of a neural network? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9dcab3",
   "metadata": {},
   "source": [
    "Divide your training set in a real training set and a validation set using one of these methods:\n",
    "\n",
    "    (k-fold / leave-one-out) Cross-validation\n",
    "    stratified holdout\n",
    "    0.632 bootstrap\n",
    "    \n",
    "Measure the performance of the ANN with one of these metrics:\n",
    "\n",
    "    TP rate\n",
    "    FP rate\n",
    "    F-measure\n",
    "    accuracy\n",
    "    precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef243bf5",
   "metadata": {},
   "source": [
    "Many methods were implemented to measure the performance of neural networks such as MSE , NMSE , RMSE, \n",
    "R square for regression. \n",
    "And TP rate ,FP rate , F-measure , accuracy , precision and recall for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8a17f",
   "metadata": {},
   "source": [
    "we r checking the performance of the neural network by this methods to get the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a8763",
   "metadata": {},
   "source": [
    "#### 3.Create a neural network using keras to predict the outcome of either of these datasets: \n",
    "Cardiac Arrhythmia: https://archive.ics.uci.edu/ml/datasets/Arrhythmia \n",
    "\n",
    "Abalone age: https://archive.ics.uci.edu/ml/datasets/Abalone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c78be1",
   "metadata": {},
   "source": [
    "#### using Ablone dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b5348b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole Weight</th>\n",
       "      <th>Schucked Weight</th>\n",
       "      <th>Viscera Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sex  Length  Diameter   Height  Whole Weight  Schucked Weight  \\\n",
       "0      M   0.455      0.365   0.095        0.5140           0.2245   \n",
       "1      M   0.350      0.265   0.090        0.2255           0.0995   \n",
       "2      F   0.530      0.420   0.135        0.6770           0.2565   \n",
       "3      M   0.440      0.365   0.125        0.5160           0.2155   \n",
       "4      I   0.330      0.255   0.080        0.2050           0.0895   \n",
       "...   ..     ...        ...     ...           ...              ...   \n",
       "4172   F   0.565      0.450   0.165        0.8870           0.3700   \n",
       "4173   M   0.590      0.440   0.135        0.9660           0.4390   \n",
       "4174   M   0.600      0.475   0.205        1.1760           0.5255   \n",
       "4175   F   0.625      0.485   0.150        1.0945           0.5310   \n",
       "4176   M   0.710      0.555   0.195        1.9485           0.9455   \n",
       "\n",
       "      Viscera Weight   Shell Weight   Rings  \n",
       "0              0.1010         0.1500     15  \n",
       "1              0.0485         0.0700      7  \n",
       "2              0.1415         0.2100      9  \n",
       "3              0.1140         0.1550     10  \n",
       "4              0.0395         0.0550      7  \n",
       "...               ...            ...    ...  \n",
       "4172           0.2390         0.2490     11  \n",
       "4173           0.2145         0.2605     10  \n",
       "4174           0.2875         0.3080      9  \n",
       "4175           0.2610         0.2960     10  \n",
       "4176           0.3765         0.4950     12  \n",
       "\n",
       "[4177 rows x 9 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1: Assessing and Processing Data\n",
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "import pandas as pd \n",
    "dataset = pd.read_csv('../week17repo/Abalone.CSV',header=None,names=['sex','Length', 'Diameter ', 'Height', 'Whole Weight', 'Schucked Weight','Viscera Weight ','Shell Weight ','Rings'])\n",
    "dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df5b157d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    1528\n",
       "I    1342\n",
       "F    1307\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abc7de83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole Weight</th>\n",
       "      <th>Schucked Weight</th>\n",
       "      <th>Viscera Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>Rings</th>\n",
       "      <th>M</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length  Diameter   Height  Whole Weight  Schucked Weight  \\\n",
       "0      0.455      0.365   0.095        0.5140           0.2245   \n",
       "1      0.350      0.265   0.090        0.2255           0.0995   \n",
       "2      0.530      0.420   0.135        0.6770           0.2565   \n",
       "3      0.440      0.365   0.125        0.5160           0.2155   \n",
       "4      0.330      0.255   0.080        0.2050           0.0895   \n",
       "...      ...        ...     ...           ...              ...   \n",
       "4172   0.565      0.450   0.165        0.8870           0.3700   \n",
       "4173   0.590      0.440   0.135        0.9660           0.4390   \n",
       "4174   0.600      0.475   0.205        1.1760           0.5255   \n",
       "4175   0.625      0.485   0.150        1.0945           0.5310   \n",
       "4176   0.710      0.555   0.195        1.9485           0.9455   \n",
       "\n",
       "      Viscera Weight   Shell Weight   Rings    M    F    I  \n",
       "0              0.1010         0.1500     15  1.0  0.0  0.0  \n",
       "1              0.0485         0.0700      7  1.0  0.0  0.0  \n",
       "2              0.1415         0.2100      9  0.0  1.0  0.0  \n",
       "3              0.1140         0.1550     10  1.0  0.0  0.0  \n",
       "4              0.0395         0.0550      7  0.0  0.0  1.0  \n",
       "...               ...            ...    ...  ...  ...  ...  \n",
       "4172           0.2390         0.2490     11  0.0  1.0  0.0  \n",
       "4173           0.2145         0.2605     10  1.0  0.0  0.0  \n",
       "4174           0.2875         0.3080      9  1.0  0.0  0.0  \n",
       "4175           0.2610         0.2960     10  0.0  1.0  0.0  \n",
       "4176           0.3765         0.4950     12  1.0  0.0  0.0  \n",
       "\n",
       "[4177 rows x 11 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One hot encoding for categorical gender variable\n",
    " \n",
    "Gender = dataset.pop('sex')\n",
    " \n",
    "dataset['M'] = (Gender == 'M')*1.0\n",
    "dataset['F'] = (Gender == 'F')*1.0\n",
    "dataset['I'] = (Gender == 'I')*1.0\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41023837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole Weight</th>\n",
       "      <th>Schucked Weight</th>\n",
       "      <th>Viscera Weight</th>\n",
       "      <th>Shell Weight</th>\n",
       "      <th>M</th>\n",
       "      <th>F</th>\n",
       "      <th>I</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length  Diameter   Height  Whole Weight  Schucked Weight  \\\n",
       "0      0.455      0.365   0.095        0.5140           0.2245   \n",
       "1      0.350      0.265   0.090        0.2255           0.0995   \n",
       "2      0.530      0.420   0.135        0.6770           0.2565   \n",
       "3      0.440      0.365   0.125        0.5160           0.2155   \n",
       "4      0.330      0.255   0.080        0.2050           0.0895   \n",
       "...      ...        ...     ...           ...              ...   \n",
       "4172   0.565      0.450   0.165        0.8870           0.3700   \n",
       "4173   0.590      0.440   0.135        0.9660           0.4390   \n",
       "4174   0.600      0.475   0.205        1.1760           0.5255   \n",
       "4175   0.625      0.485   0.150        1.0945           0.5310   \n",
       "4176   0.710      0.555   0.195        1.9485           0.9455   \n",
       "\n",
       "      Viscera Weight   Shell Weight     M    F    I  Rings  \n",
       "0              0.1010         0.1500  1.0  0.0  0.0     15  \n",
       "1              0.0485         0.0700  1.0  0.0  0.0      7  \n",
       "2              0.1415         0.2100  0.0  1.0  0.0      9  \n",
       "3              0.1140         0.1550  1.0  0.0  0.0     10  \n",
       "4              0.0395         0.0550  0.0  0.0  1.0      7  \n",
       "...               ...            ...  ...  ...  ...    ...  \n",
       "4172           0.2390         0.2490  0.0  1.0  0.0     11  \n",
       "4173           0.2145         0.2605  1.0  0.0  0.0     10  \n",
       "4174           0.2875         0.3080  1.0  0.0  0.0      9  \n",
       "4175           0.2610         0.2960  0.0  1.0  0.0     10  \n",
       "4176           0.3765         0.4950  1.0  0.0  0.0     12  \n",
       "\n",
       "[4177 rows x 11 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reorder Columns\n",
    " \n",
    "dataset = dataset[['Length', 'Diameter ', 'Height', 'Whole Weight', 'Schucked Weight','Viscera Weight ','Shell Weight ','M','F','I','Rings']]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "337762d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate input data and labels\n",
    "X=dataset.iloc[:,0:10]\n",
    "y=dataset.iloc[:,10].values\n",
    " \n",
    "#Normalize the data using the min-max scalar\n",
    " \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scalar= MinMaxScaler()\n",
    "X= scalar.fit_transform(X)\n",
    "y= y.reshape(-1,1)\n",
    "y=scalar.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820a9fd",
   "metadata": {},
   "source": [
    "We then split the data into a training set that is composed of 80% of the dataset. The remaining 20% is the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76758c15",
   "metadata": {},
   "source": [
    "Building and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c20514",
   "metadata": {},
   "source": [
    "Then we build the Keras structure. The core of this structure is the model, which is of the “Sequential” form. This is the simplest style of model and is composed of a linear stack of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bf9cd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "535/535 [==============================] - 2s 4ms/step - loss: 0.0112 - mae: 0.0764 - mse: 0.0112 - val_loss: 0.0080 - val_mae: 0.0618 - val_mse: 0.0080\n",
      "Epoch 2/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0081 - mae: 0.0639 - mse: 0.0081 - val_loss: 0.0072 - val_mae: 0.0592 - val_mse: 0.0072\n",
      "Epoch 3/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0075 - mae: 0.0614 - mse: 0.0075 - val_loss: 0.0067 - val_mae: 0.0585 - val_mse: 0.0067\n",
      "Epoch 4/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0072 - mae: 0.0600 - mse: 0.0072 - val_loss: 0.0068 - val_mae: 0.0635 - val_mse: 0.0068\n",
      "Epoch 5/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0071 - mae: 0.0598 - mse: 0.0071 - val_loss: 0.0064 - val_mae: 0.0555 - val_mse: 0.0064\n",
      "Epoch 6/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0069 - mae: 0.0585 - mse: 0.0069 - val_loss: 0.0062 - val_mae: 0.0588 - val_mse: 0.0062\n",
      "Epoch 7/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0068 - mae: 0.0583 - mse: 0.0068 - val_loss: 0.0060 - val_mae: 0.0567 - val_mse: 0.0060\n",
      "Epoch 8/100\n",
      "535/535 [==============================] - 2s 4ms/step - loss: 0.0066 - mae: 0.0581 - mse: 0.0066 - val_loss: 0.0060 - val_mae: 0.0570 - val_mse: 0.0060\n",
      "Epoch 9/100\n",
      "535/535 [==============================] - 2s 4ms/step - loss: 0.0066 - mae: 0.0582 - mse: 0.0066 - val_loss: 0.0059 - val_mae: 0.0552 - val_mse: 0.0059\n",
      "Epoch 10/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0065 - mae: 0.0576 - mse: 0.0065 - val_loss: 0.0059 - val_mae: 0.0547 - val_mse: 0.0059\n",
      "Epoch 11/100\n",
      "535/535 [==============================] - 2s 4ms/step - loss: 0.0065 - mae: 0.0572 - mse: 0.0065 - val_loss: 0.0062 - val_mae: 0.0586 - val_mse: 0.0062\n",
      "Epoch 12/100\n",
      "535/535 [==============================] - 2s 5ms/step - loss: 0.0065 - mae: 0.0576 - mse: 0.0065 - val_loss: 0.0060 - val_mae: 0.0543 - val_mse: 0.0060\n",
      "Epoch 13/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0064 - mae: 0.0573 - mse: 0.0064 - val_loss: 0.0062 - val_mae: 0.0595 - val_mse: 0.0062\n",
      "Epoch 14/100\n",
      "535/535 [==============================] - 1s 2ms/step - loss: 0.0065 - mae: 0.0575 - mse: 0.0065 - val_loss: 0.0089 - val_mae: 0.0773 - val_mse: 0.0089\n",
      "Epoch 15/100\n",
      "535/535 [==============================] - 1s 3ms/step - loss: 0.0065 - mae: 0.0579 - mse: 0.0065 - val_loss: 0.0061 - val_mae: 0.0548 - val_mse: 0.0061\n",
      "Epoch 16/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0063 - mae: 0.0566 - mse: 0.0063 - val_loss: 0.0059 - val_mae: 0.0558 - val_mse: 0.0059\n",
      "Epoch 17/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0064 - mae: 0.0570 - mse: 0.0064 - val_loss: 0.0064 - val_mae: 0.0603 - val_mse: 0.0064\n",
      "Epoch 18/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0064 - mae: 0.0567 - mse: 0.0064 - val_loss: 0.0058 - val_mae: 0.0558 - val_mse: 0.0058\n",
      "Epoch 19/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0064 - mae: 0.0574 - mse: 0.0064 - val_loss: 0.0059 - val_mae: 0.0542 - val_mse: 0.0059\n",
      "Epoch 20/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0064 - mae: 0.0573 - mse: 0.0064 - val_loss: 0.0060 - val_mae: 0.0554 - val_mse: 0.0060\n",
      "Epoch 21/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0063 - mae: 0.0567 - mse: 0.0063 - val_loss: 0.0064 - val_mae: 0.0608 - val_mse: 0.0064\n",
      "Epoch 22/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0064 - mae: 0.0568 - mse: 0.0064 - val_loss: 0.0059 - val_mae: 0.0542 - val_mse: 0.0059\n",
      "Epoch 23/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0063 - mae: 0.0565 - mse: 0.0063 - val_loss: 0.0063 - val_mae: 0.0595 - val_mse: 0.0063\n",
      "Epoch 24/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0063 - mae: 0.0565 - mse: 0.0063 - val_loss: 0.0062 - val_mae: 0.0585 - val_mse: 0.0062\n",
      "Epoch 25/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0063 - mae: 0.0566 - mse: 0.0063 - val_loss: 0.0061 - val_mae: 0.0587 - val_mse: 0.0061\n",
      "Epoch 26/100\n",
      "535/535 [==============================] - 2s 4ms/step - loss: 0.0063 - mae: 0.0562 - mse: 0.0063 - val_loss: 0.0059 - val_mae: 0.0569 - val_mse: 0.0059\n",
      "Epoch 27/100\n",
      "535/535 [==============================] - 2s 3ms/step - loss: 0.0063 - mae: 0.0563 - mse: 0.0063 - val_loss: 0.0059 - val_mae: 0.0547 - val_mse: 0.0059\n",
      "Epoch 28/100\n",
      "535/535 [==============================] - 2s 4ms/step - loss: 0.0062 - mae: 0.0563 - mse: 0.0062 - val_loss: 0.0058 - val_mae: 0.0552 - val_mse: 0.0058\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build Keras Model\n",
    " \n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, input_dim=10,activation='relu'))\n",
    "model.add(Dense(units=1,activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae','mse'])\n",
    " \n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    " \n",
    "history=model.fit(X_train,y_train,batch_size=5, validation_split = 0.2, callbacks=[early_stop], epochs=100)\n",
    " \n",
    "# Model summary for number of parameters use in the algorithm\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5b45b",
   "metadata": {},
   "source": [
    "Stacking layers is executed with model.add(). We stack a dense layer of 10 nodes that has 10 inputs feeding into the layer. The activation function chosen for this layer is a ReLU (Rectified Linear Unit), a popular choice due to better gradient propagation and sparser activation than a sigmoidal function for example. A final output layer with a linear activation function is stacked to simply return the model output. This network architecture was picked rather arbitrarily, but can be tuned to achieve better performance. The model is compiled using model.compile(). Here, we specify the type of optimizer- in this case the Adam optimizer which has become a popular alternative to the more traditional stochastic gradient descent. A mean-squared-error loss function is specified and the metrics reported will be “mean squared error” and “mean absolute error”. Then we call model.fit() to iterate training in batch sizes. Batch size corresponds to the number of training instances that are processed before the model is updated. The number of epochs is the number of complete passes through the dataset. The more times that the model can see the dataset, the more chances it has to learn the patterns, but too many epochs can also lead to overfitting. The number of epochs appropriate for this case is unknown, so I can implement a validation set that is 20% of my current training set. I set a large value of 100 epochs and add early stopping criteria that stops training when the validation score stops improving and helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b6bf6",
   "metadata": {},
   "source": [
    "Training and validation error can be plotted as a function of epochs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe7852",
   "metadata": {},
   "source": [
    " Prediction and Assessment of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc27b0",
   "metadata": {},
   "source": [
    "Once our model is trained, we can use it to predict the age of the abalone in the test set. Once the values are predicted, then they must be re-scaled back which is performed using the inverse_transform function from Scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17dd17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict testing labels\n",
    " \n",
    "y_pred= model.predict(X_test)\n",
    " \n",
    "#undo normalization \n",
    " \n",
    "y_pred_transformed=scalar.inverse_transform(y_pred.reshape(-1,1))\n",
    "y_test_transformed=scalar.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0b52987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.072954813813219\n",
      "0.5452074850270103\n"
     ]
    }
   ],
   "source": [
    "#visualize performance\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test_transformed, y_pred_transformed)\n",
    "ax.plot([y_test_transformed.min(), y_test_transformed.max()], [y_test_transformed.min(), y_test_transformed.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured (Rings)')\n",
    "ax.set_ylabel('Predicted (Rings)')\n",
    "plt.show()\n",
    " \n",
    "#Calculate RMSE and R^2\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "rms = sqrt(mean_squared_error(y_test_transformed, y_pred_transformed))\n",
    "print(rms)\n",
    " \n",
    "from sklearn.metrics import r2_score\n",
    "r_squared=r2_score(y_test_transformed,y_pred_transformed)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de375165",
   "metadata": {},
   "source": [
    "The performance is not ideal, but the results are appropriate given that this dataset is notoriously hard to use for prediction without other relevant information such as weather or location. Ultimately, it is up to the user to decide if these are significant/acceptable values, otherwise the neural network hyperparameters can be further fine-tuned or more input data and more features can be added to the dataset to try to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cffec5",
   "metadata": {},
   "source": [
    "#### 4.Write another algorithm to predict the same result as the previous question using either KNN or logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad446f31",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d72964b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = dataset[\"Rings\"]\n",
    "#X = dataset.drop(columns=\"Rings\")\n",
    "\n",
    "#Separate input data and labels\n",
    "X=dataset.iloc[:,0:8]\n",
    "y=dataset.iloc[:,8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aaf83d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3427768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8660287081339713\n",
      "0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Initializing classifier and giving hyperparameter k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "# training classifier\n",
    "knn.fit(X_train, y_train)\n",
    "# Evaluate the classifier\n",
    "print(knn.score(X_test, y_test))\n",
    "# Try changing hyperparameter\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ee149",
   "metadata": {},
   "source": [
    "We get accuracy of 0.64 and 0.62 for k=3 and k=5 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c74536",
   "metadata": {},
   "source": [
    "#### Algorithm 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad4c3716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8600478468899522\n"
     ]
    }
   ],
   "source": [
    " from sklearn.linear_model import LogisticRegression\n",
    "# Initializing classifier with one-v-rest approach. random_state is # to ensure same results in every execution.\n",
    "logr = LogisticRegression(multi_class = 'ovr', random_state=3)\n",
    "# training classifier\n",
    "logr.fit(X_train, y_train)\n",
    "# Evaluate the classifier\n",
    "print(logr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49c640",
   "metadata": {},
   "source": [
    "#### 5.Create a neural network using pytorch to predict the same result as question 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87c84600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177, 11)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "426290f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate input data and labels\n",
    "X=dataset.iloc[:,0:8]\n",
    "y=dataset.iloc[:,8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b48a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
    "\n",
    "# #Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6af68a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7188,  0.7268,  0.6020,  ...,  0.7055,  0.4721,  1.3075],\n",
      "        [ 0.3016,  0.4239,  0.4832,  ...,  0.8066,  0.4362, -0.7648],\n",
      "        [ 0.2598,  0.3229,  0.2458,  ..., -0.3153, -0.0660,  1.3075],\n",
      "        ...,\n",
      "        [-0.5329, -0.4847, -0.9414,  ..., -0.8027, -0.8909, -0.7648],\n",
      "        [-0.9084, -0.8380, -0.9414,  ..., -1.0004, -1.0344, -0.7648],\n",
      "        [ 0.5102,  0.4239,  0.4832,  ...,  0.9860,  0.0775, -0.7648]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #this has activation functions\n",
    "\n",
    "# Creating tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d3109247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Model(nn.Module):\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=20, out_features =2):\n",
    "        super().__init__()\n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd797cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ANN_Model(\n",
       "  (layer_1_connection): Linear(in_features=8, out_features=20, bias=True)\n",
       "  (layer_2_connection): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (out): Linear(in_features=20, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#instantiate the model\n",
    "model = ANN_Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b50c9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e319e16",
   "metadata": {},
   "source": [
    "#Model Training\n",
    "\n",
    "This part will also be extremely simple. We’ll train the model for 100 epochs, keeping track of time and loss. Every 10 epochs we’ll output to the console the current status — indicating on which epoch are we and what’s the current loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35fda472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 with loss: 0.6248833537101746\n",
      "Epoch number: 11 with loss: 0.46087217330932617\n",
      "Epoch number: 21 with loss: 0.2841832935810089\n",
      "Epoch number: 31 with loss: 0.2739645540714264\n",
      "Epoch number: 41 with loss: 0.2651500403881073\n",
      "Epoch number: 51 with loss: 0.26045286655426025\n",
      "Epoch number: 61 with loss: 0.2557704746723175\n",
      "Epoch number: 71 with loss: 0.251229852437973\n",
      "Epoch number: 81 with loss: 0.24725529551506042\n",
      "Epoch number: 91 with loss: 0.24399934709072113\n"
     ]
    }
   ],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = model.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        print(f'Epoch number: {epoch} with loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad() #zero the gradient before running backwards propagation\n",
    "    loss.backward() #for backward propagation \n",
    "    optimizer.step() #performs one optimization step each epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "11e825b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = model(data)\n",
    "        y_pred.append(prediction.argmax().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecc0a33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8911483253588517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "a_score = accuracy_score(y_test, y_pred)\n",
    "print(a_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "198f1328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       574\n",
      "           1       0.84      0.80      0.82       262\n",
      "\n",
      "    accuracy                           0.89       836\n",
      "   macro avg       0.88      0.87      0.87       836\n",
      "weighted avg       0.89      0.89      0.89       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db87d2b",
   "metadata": {},
   "source": [
    "#### 6.Compare the performance of the neural networks to the other model you created. Which performed better? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84361d56",
   "metadata": {},
   "source": [
    "with the keras model we get the accuracy of model is 0.54%, with the KNN and Logistic Regression we get the accuracy 0.86% \n",
    "and using neural network with pytorch we get the highest accuracy is 0.89%. \n",
    "so neural network with pytorch performed better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
